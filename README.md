# ğŸ©º RAG-Based-Medical-Chatbot
A medical chatbot using RAG with HuggingFace, FAISS, LangChain, and Streamlit. Powered by llama-2-7b-chat.Q4_K_M.gguf, a 7B dialogue-optimized LLaMA 2 model in GGUF format, it retrieves context from medical PDFs and generates accurate, AI-based answers.

---

## ğŸ§  About the Model

We use `llama-2-7b-chat.Q4_K_M.gguf`, a 7B parameter model from LLaMA 2â€”optimized for dialogue and converted to GGUF format for efficient local inference. LLaMA 2 is a family of pretrained and fine-tuned generative text models ranging from 7B to 70B parameters.

---

## âš™ï¸ Features

- ğŸ“„ Parse and embed medical PDF documents
- ğŸ” Semantic search with FAISS
- ğŸ§© Modular query processing via LangChain
- ğŸ¤– Local LLM inference using LLaMA 2 in GGUF format
- ğŸ’¬ Streamlit-based chatbot interface

---

## ğŸ§° Tech Stack

- Python
- HuggingFace Transformers
- FAISS
- LangChain
- llama-cpp-python
- Streamlit
- Jupyter notebook (optional)

---

## ğŸ“‚ Folder Structure
.
â”œâ”€â”€ data/
    â””â”€â”€ downloadmedicalbook.txt            # Raw text data from medical book
â”œâ”€â”€ model/
â”‚   â””â”€â”€ instructions.txt                   # Instructions or metadata for the model
â”œâ”€â”€ .gitignore                             # Git ignore file
â”œâ”€â”€ LICENSE                                # MIT License
â”œâ”€â”€ README.md                              # Project overview and details
â”œâ”€â”€ connect_memory_with_llm.ipynb          # Notebook to connect vector memory with LLM
â”œâ”€â”€ create_memory_for_llm.ipynb            # Notebook to create memory using FAISS
â”œâ”€â”€ medibot.py                             # Main chatbot logic
â”œâ”€â”€ requirements.txt                       # Required Python packages




