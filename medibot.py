{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit langchain langchain-community langchain-huggingface ctransformers sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf46fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:45:09.637 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:09.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:09.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.451 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.452 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.454 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-02 21:45:10.455 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "# === PATHS ===\n",
    "LLAMA_MODEL_PATH = \"C:/Users/chill/llama/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# === Load Local LLaMA-2 Model ===\n",
    "@st.cache_resource\n",
    "def load_local_llama_model():\n",
    "    llm = CTransformers(\n",
    "        model=LLAMA_MODEL_PATH,\n",
    "        model_type=\"llama\",\n",
    "        config={\n",
    "            \"max_new_tokens\": 100,  # Optimized for speed\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "# === Load Embedding Model ===\n",
    "@st.cache_resource\n",
    "def load_embedding_model():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    return embedding_model\n",
    "\n",
    "# === Load Vector Database ===\n",
    "def load_vector_db(_embedding_model):\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings=_embedding_model, allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "# === Build Prompt for LLaMA-2 ===\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "<s>[INST] Use the context to answer: {context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# === Generate Answer using LLaMA-2 ===\n",
    "def generate_answer(llm, prompt):\n",
    "    answer = llm(prompt)  # CTransformers directly returns the answer string\n",
    "    return answer\n",
    "\n",
    "# === Streamlit App ===\n",
    "def main():\n",
    "    st.title(\"ðŸš€ Local LLaMA-2 Chatbot (NO API)\")\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # Load models and DB\n",
    "    llm = load_local_llama_model()\n",
    "    embedding_model = load_embedding_model()\n",
    "    vector_db = load_vector_db(embedding_model)\n",
    "\n",
    "    # Display previous messages\n",
    "    for message in st.session_state.messages:\n",
    "        st.chat_message(message[\"role\"]).markdown(message[\"content\"])\n",
    "\n",
    "    # User input\n",
    "    user_input = st.chat_input(\"Ask your question here...\")\n",
    "\n",
    "    if user_input:\n",
    "        st.chat_message(\"user\").markdown(user_input)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Retrieve top document (k=1 for token limit)\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "        docs = retriever.get_relevant_documents(user_input)\n",
    "\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        full_prompt = build_prompt(context, user_input)\n",
    "\n",
    "        # Generate answer\n",
    "        answer = generate_answer(llm, full_prompt)\n",
    "\n",
    "        st.chat_message(\"assistant\").markdown(answer)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        # Optional: show sources\n",
    "        if docs:\n",
    "            st.markdown(\"**Sources Used:**\")\n",
    "            for idx, doc in enumerate(docs, start=1):\n",
    "                st.markdown(f\"**Source {idx}:** {doc.metadata}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medical-Chatbot-iHinoZwD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
